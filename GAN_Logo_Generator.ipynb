{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "GAN Logo Generator",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LavDeshpande/Chatbot-Question-and-Answer/blob/main/GAN_Logo_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8_-1h5ddiDp",
        "outputId": "a31e92dd-4a62-4242-acc2-c17b7fa14146"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KubxTY1mdiDm"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb_XblE7diDr"
      },
      "source": [
        "GENERATE_RES = 4 \n",
        "GENERATE_SQUARE = 32 * GENERATE_RES \n",
        "IMAGE_CHANNELS = 3\n",
        "PREVIEW_ROWS = 4\n",
        "PREVIEW_COLS = 7\n",
        "PREVIEW_MARGIN = 16\n",
        "SEED_SIZE = 100\n",
        "DATA_PATH =  '/content/drive/MyDrive/globalsummitlogogan-master-LogoImages/Project'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 60000"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhapUrdYyGLL"
      },
      "source": [
        "GENERATE_RES = 6\n",
        "GENERATE_SQUARE = 32 * GENERATE_RES \n",
        "IMAGE_CHANNELS = 3\n",
        "PREVIEW_ROWS = 4\n",
        "PREVIEW_COLS = 7\n",
        "PREVIEW_MARGIN = 16\n",
        "SEED_SIZE = 100\n",
        "DATA_PATH =  '/content/drive/MyDrive/globalsummitlogogan-master-LogoImages/proj'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 60000"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ69ALfSdiDv"
      },
      "source": [
        "training_binary_path = os.path.join(DATA_PATH,\n",
        "        f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n",
        "\n",
        "if not os.path.isfile(training_binary_path):\n",
        "  start = time.time()\n",
        "  training_data = []\n",
        "  logo_path='/content/drive/MyDrive/globalsummitlogogan-master-LogoImages/Project/LogoImages'\n",
        "  for filename in tqdm(os.listdir(logo_path)):\n",
        "      if filename != '.ipynb_checkpoints':\n",
        "        path = os.path.join(logo_path,filename)\n",
        "        image = Image.open(path).resize((GENERATE_SQUARE,\n",
        "              GENERATE_SQUARE),3)\n",
        "        if np.asarray(image).shape == (128,128,3):\n",
        "          training_data.append(np.asarray(image))\n",
        "  training_data = np.reshape(training_data,(-1,GENERATE_SQUARE,\n",
        "            GENERATE_SQUARE,IMAGE_CHANNELS))\n",
        "  training_data = training_data.astype(np.float32)\n",
        "  training_data = training_data / 127.5 - 1.\n",
        "\n",
        "  np.save(training_binary_path,training_data)\n",
        "  elapsed = time.time()-start\n",
        "else:\n",
        "  training_data = np.load(training_binary_path)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbUkLaB_yQ6k",
        "outputId": "6477dfba-6639-484f-ce71-2c23c165fdd7"
      },
      "source": [
        "training_binary_path = os.path.join(DATA_PATH,\n",
        "        f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n",
        "\n",
        "if not os.path.isfile(training_binary_path):\n",
        "  start = time.time()\n",
        "  training_data = []\n",
        "  logo_path='/content/drive/MyDrive/globalsummitlogogan-master-LogoImages/proj/Logo'\n",
        "  for filename in tqdm(os.listdir(logo_path)):\n",
        "      if filename != '.ipynb_checkpoints':\n",
        "        path = os.path.join(logo_path,filename)\n",
        "        image = Image.open(path).resize((GENERATE_SQUARE,\n",
        "              GENERATE_SQUARE),3)\n",
        "        if np.asarray(image).shape == (192,192,3):\n",
        "          training_data.append(np.asarray(image))\n",
        "  training_data = np.reshape(training_data,(-1,GENERATE_SQUARE,\n",
        "            GENERATE_SQUARE,IMAGE_CHANNELS))\n",
        "  training_data = training_data.astype(np.float32)\n",
        "  training_data = training_data / 127.5 - 1.\n",
        "\n",
        "  np.save(training_binary_path,training_data)\n",
        "  elapsed = time.time()-start\n",
        "else:\n",
        "  training_data = np.load(training_binary_path)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 93/93 [00:00<00:00, 417.11it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXl0JohJBx69"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(training_data) \\\n",
        "    .shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r4FzaDg4q_u",
        "outputId": "0b2b5eef-d847-42ab-dade-f1163279267c"
      },
      "source": [
        "for i in train_dataset:\n",
        "  print('i')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulou-BZPybzT"
      },
      "source": [
        "def build_generator(seed_size, channels):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n",
        "    model.add(Reshape((4,4,256)))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "   \n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    if GENERATE_RES>1:\n",
        "      model.add(UpSampling2D(size=(GENERATE_RES,GENERATE_RES)))\n",
        "      model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
        "      model.add(BatchNormalization(momentum=0.8))\n",
        "      model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_discriminator(image_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, \n",
        "                     padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKnCeDut2cp0"
      },
      "source": [
        "def save_images(cnt,noise):\n",
        "  image_array = np.full(( \n",
        "      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n",
        "      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), 3), \n",
        "      255, dtype=np.uint8)\n",
        "  \n",
        "  generated_images = generator.predict(noise)\n",
        "\n",
        "  generated_images = 0.5 * generated_images + 0.5\n",
        "\n",
        "  image_count = 0\n",
        "  for row in range(PREVIEW_ROWS):\n",
        "      for col in range(PREVIEW_COLS):\n",
        "        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \\\n",
        "            = generated_images[image_count] * 255\n",
        "        image_count += 1\n",
        "\n",
        "          \n",
        "  output_path = os.path.join(DATA_PATH,'output')\n",
        "  if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "  \n",
        "  filename = os.path.join(output_path,f\"train-{cnt}.png\")\n",
        "  im = Image.fromarray(image_array)\n",
        "  im.save(filename)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVx-6F-Yyub_"
      },
      "source": [
        "generator = build_generator(SEED_SIZE, IMAGE_CHANNELS)\n",
        "noise = tf.random.normal([1, SEED_SIZE])\n",
        "generated_image = generator(noise, training=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqld2vQ2AQzh",
        "outputId": "881de655-f567-407a-d281-7b134bd7ba36"
      },
      "source": [
        "generated_image.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 192, 192, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHI9Q7jOy1Vo"
      },
      "source": [
        "image_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\n",
        "discriminator = build_discriminator(image_shape)\n",
        "decision = discriminator(generated_image)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBaP98zAySJV"
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79UDhOCa0R4h"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzyh-LqU0j5d"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_images = generator(seed, training=True)\n",
        "\n",
        "    real_output = discriminator(images, training=True)\n",
        "    fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    \n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(\\\n",
        "        gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(\\\n",
        "        disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(\n",
        "        gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(\n",
        "        gradients_of_discriminator, \n",
        "        discriminator.trainable_variables))\n",
        "  return gen_loss,disc_loss"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjrRgDR10lSF"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, \n",
        "                                       SEED_SIZE))\n",
        "  start = time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    gen_loss_list = []\n",
        "    disc_loss_list = []\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      t = train_step(image_batch)\n",
        "      gen_loss_list.append(t[0])\n",
        "      disc_loss_list.append(t[1])\n",
        "\n",
        "    g_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
        "    d_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
        "\n",
        "    epoch_elapsed = time.time()-epoch_start\n",
        "    print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}')\n",
        "    save_images(epoch,fixed_seed)\n",
        "\n",
        "  elapsed = time.time()-start"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vWmEHprD0t1V",
        "outputId": "00efdaee-612b-4640-d43c-242558db48c5"
      },
      "source": [
        "train(train_dataset, 1000)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, gen loss=0.9121295809745789,disc loss=1.5809259414672852\n",
            "Epoch 2, gen loss=1.336830973625183,disc loss=1.263245940208435\n",
            "Epoch 3, gen loss=1.0024633407592773,disc loss=1.4497228860855103\n",
            "Epoch 4, gen loss=1.3940109014511108,disc loss=1.0493930578231812\n",
            "Epoch 5, gen loss=1.3679736852645874,disc loss=1.0387405157089233\n",
            "Epoch 6, gen loss=1.4613007307052612,disc loss=0.7376692891120911\n",
            "Epoch 7, gen loss=1.3674930334091187,disc loss=0.9494383931159973\n",
            "Epoch 8, gen loss=1.033138394355774,disc loss=1.5201376676559448\n",
            "Epoch 9, gen loss=1.3381487131118774,disc loss=0.9542720913887024\n",
            "Epoch 10, gen loss=1.8241618871688843,disc loss=0.6460769772529602\n",
            "Epoch 11, gen loss=1.3421363830566406,disc loss=1.3918342590332031\n",
            "Epoch 12, gen loss=0.920189619064331,disc loss=1.1179667711257935\n",
            "Epoch 13, gen loss=1.2947181463241577,disc loss=1.3044229745864868\n",
            "Epoch 14, gen loss=1.307655692100525,disc loss=1.3647111654281616\n",
            "Epoch 15, gen loss=1.189254641532898,disc loss=1.0523664951324463\n",
            "Epoch 16, gen loss=1.1845816373825073,disc loss=1.3450287580490112\n",
            "Epoch 17, gen loss=0.7719413638114929,disc loss=1.726314902305603\n",
            "Epoch 18, gen loss=0.8532856106758118,disc loss=1.7212098836898804\n",
            "Epoch 19, gen loss=1.5260658264160156,disc loss=1.0531848669052124\n",
            "Epoch 20, gen loss=1.7843408584594727,disc loss=0.6815131306648254\n",
            "Epoch 21, gen loss=0.924060046672821,disc loss=1.4428333044052124\n",
            "Epoch 22, gen loss=1.3098613023757935,disc loss=1.7434028387069702\n",
            "Epoch 23, gen loss=1.5398916006088257,disc loss=1.5421215295791626\n",
            "Epoch 24, gen loss=0.840499222278595,disc loss=2.211780548095703\n",
            "Epoch 25, gen loss=1.5390952825546265,disc loss=1.5973706245422363\n",
            "Epoch 26, gen loss=1.3531302213668823,disc loss=0.7746431231498718\n",
            "Epoch 27, gen loss=1.708723545074463,disc loss=0.6960673332214355\n",
            "Epoch 28, gen loss=1.3739198446273804,disc loss=1.1851142644882202\n",
            "Epoch 29, gen loss=1.1877402067184448,disc loss=1.219997763633728\n",
            "Epoch 30, gen loss=1.058463454246521,disc loss=1.585077166557312\n",
            "Epoch 31, gen loss=0.9108783602714539,disc loss=1.7563832998275757\n",
            "Epoch 32, gen loss=1.3148545026779175,disc loss=1.7324100732803345\n",
            "Epoch 33, gen loss=1.478658676147461,disc loss=0.8396392464637756\n",
            "Epoch 34, gen loss=1.6566799879074097,disc loss=0.6791632771492004\n",
            "Epoch 35, gen loss=0.9958839416503906,disc loss=1.3084006309509277\n",
            "Epoch 36, gen loss=1.3000215291976929,disc loss=1.3873305320739746\n",
            "Epoch 37, gen loss=1.8370672464370728,disc loss=0.7330185770988464\n",
            "Epoch 38, gen loss=1.4382696151733398,disc loss=0.8935174942016602\n",
            "Epoch 39, gen loss=1.4945732355117798,disc loss=0.8020949363708496\n",
            "Epoch 40, gen loss=1.21930730342865,disc loss=1.5459562540054321\n",
            "Epoch 41, gen loss=1.4900946617126465,disc loss=1.5139344930648804\n",
            "Epoch 42, gen loss=1.910607933998108,disc loss=0.44560179114341736\n",
            "Epoch 43, gen loss=1.4764186143875122,disc loss=0.7688067555427551\n",
            "Epoch 44, gen loss=1.3077887296676636,disc loss=1.2672253847122192\n",
            "Epoch 45, gen loss=1.255707859992981,disc loss=1.611058235168457\n",
            "Epoch 46, gen loss=2.4826841354370117,disc loss=1.0405594110488892\n",
            "Epoch 47, gen loss=1.4039835929870605,disc loss=1.1694260835647583\n",
            "Epoch 48, gen loss=1.1895631551742554,disc loss=1.4611982107162476\n",
            "Epoch 49, gen loss=1.2106865644454956,disc loss=1.041789174079895\n",
            "Epoch 50, gen loss=2.0502142906188965,disc loss=0.5743148326873779\n",
            "Epoch 51, gen loss=1.6328402757644653,disc loss=0.7148060202598572\n",
            "Epoch 52, gen loss=2.1737775802612305,disc loss=0.7763387560844421\n",
            "Epoch 53, gen loss=2.3473525047302246,disc loss=0.3800986707210541\n",
            "Epoch 54, gen loss=2.2433507442474365,disc loss=0.40677952766418457\n",
            "Epoch 55, gen loss=2.0508840084075928,disc loss=0.9632899165153503\n",
            "Epoch 56, gen loss=2.4351441860198975,disc loss=0.7477161884307861\n",
            "Epoch 57, gen loss=1.5175892114639282,disc loss=1.010527491569519\n",
            "Epoch 58, gen loss=1.578346848487854,disc loss=1.0969473123550415\n",
            "Epoch 59, gen loss=2.127514600753784,disc loss=0.47828376293182373\n",
            "Epoch 60, gen loss=2.5919978618621826,disc loss=0.26495179533958435\n",
            "Epoch 61, gen loss=3.236518621444702,disc loss=0.1739138960838318\n",
            "Epoch 62, gen loss=1.5921143293380737,disc loss=0.9696200489997864\n",
            "Epoch 63, gen loss=3.2697079181671143,disc loss=0.7237932682037354\n",
            "Epoch 64, gen loss=1.5126018524169922,disc loss=1.2097249031066895\n",
            "Epoch 65, gen loss=2.247175931930542,disc loss=1.4374898672103882\n",
            "Epoch 66, gen loss=1.760928988456726,disc loss=1.3049912452697754\n",
            "Epoch 67, gen loss=2.1450719833374023,disc loss=1.1789697408676147\n",
            "Epoch 68, gen loss=2.1166648864746094,disc loss=0.4763396680355072\n",
            "Epoch 69, gen loss=2.007601737976074,disc loss=0.3908442556858063\n",
            "Epoch 70, gen loss=2.1627187728881836,disc loss=0.6151083111763\n",
            "Epoch 71, gen loss=2.206648588180542,disc loss=0.4883337914943695\n",
            "Epoch 72, gen loss=1.9154363870620728,disc loss=0.9236970543861389\n",
            "Epoch 73, gen loss=1.7113310098648071,disc loss=0.726963460445404\n",
            "Epoch 74, gen loss=1.6715189218521118,disc loss=1.1343955993652344\n",
            "Epoch 75, gen loss=2.333238363265991,disc loss=0.734351396560669\n",
            "Epoch 76, gen loss=1.8020278215408325,disc loss=0.6736076474189758\n",
            "Epoch 77, gen loss=2.6398937702178955,disc loss=0.5583946108818054\n",
            "Epoch 78, gen loss=2.0584795475006104,disc loss=0.5836293697357178\n",
            "Epoch 79, gen loss=2.0826187133789062,disc loss=0.5852674841880798\n",
            "Epoch 80, gen loss=2.9862148761749268,disc loss=0.28814464807510376\n",
            "Epoch 81, gen loss=2.984516143798828,disc loss=0.2373976707458496\n",
            "Epoch 82, gen loss=2.4027671813964844,disc loss=0.568959653377533\n",
            "Epoch 83, gen loss=2.512866258621216,disc loss=0.3224087655544281\n",
            "Epoch 84, gen loss=1.980015754699707,disc loss=0.9589177966117859\n",
            "Epoch 85, gen loss=2.505403757095337,disc loss=0.482531875371933\n",
            "Epoch 86, gen loss=1.6802610158920288,disc loss=1.09611976146698\n",
            "Epoch 87, gen loss=2.404820203781128,disc loss=0.9893298149108887\n",
            "Epoch 88, gen loss=1.6876312494277954,disc loss=1.208659291267395\n",
            "Epoch 89, gen loss=1.4326788187026978,disc loss=1.4104052782058716\n",
            "Epoch 90, gen loss=1.786944031715393,disc loss=1.4855576753616333\n",
            "Epoch 91, gen loss=2.055921792984009,disc loss=0.609390139579773\n",
            "Epoch 92, gen loss=2.5008704662323,disc loss=0.3985372483730316\n",
            "Epoch 93, gen loss=2.670398473739624,disc loss=0.2765751779079437\n",
            "Epoch 94, gen loss=2.606417179107666,disc loss=0.2938975393772125\n",
            "Epoch 95, gen loss=2.2257893085479736,disc loss=0.5183623433113098\n",
            "Epoch 96, gen loss=1.5244041681289673,disc loss=1.0890339612960815\n",
            "Epoch 97, gen loss=3.435938596725464,disc loss=0.9839733242988586\n",
            "Epoch 98, gen loss=2.1113083362579346,disc loss=0.6008678078651428\n",
            "Epoch 99, gen loss=3.203561782836914,disc loss=0.699902355670929\n",
            "Epoch 100, gen loss=1.757990837097168,disc loss=0.7928679585456848\n",
            "Epoch 101, gen loss=3.1780452728271484,disc loss=0.45832860469818115\n",
            "Epoch 102, gen loss=2.7092397212982178,disc loss=0.4868645966053009\n",
            "Epoch 103, gen loss=1.6641160249710083,disc loss=0.9838760495185852\n",
            "Epoch 104, gen loss=1.8273369073867798,disc loss=0.9659819006919861\n",
            "Epoch 105, gen loss=2.375480890274048,disc loss=0.40370652079582214\n",
            "Epoch 106, gen loss=2.26358699798584,disc loss=0.5257362723350525\n",
            "Epoch 107, gen loss=2.2309234142303467,disc loss=0.4973306655883789\n",
            "Epoch 108, gen loss=1.2623730897903442,disc loss=1.2830241918563843\n",
            "Epoch 109, gen loss=1.9296221733093262,disc loss=1.3918207883834839\n",
            "Epoch 110, gen loss=1.969810128211975,disc loss=1.1517361402511597\n",
            "Epoch 111, gen loss=2.4604063034057617,disc loss=0.5595082640647888\n",
            "Epoch 112, gen loss=2.961915969848633,disc loss=0.34780195355415344\n",
            "Epoch 113, gen loss=1.775009274482727,disc loss=0.7204400897026062\n",
            "Epoch 114, gen loss=2.265737533569336,disc loss=1.3655166625976562\n",
            "Epoch 115, gen loss=3.593479871749878,disc loss=0.5430485010147095\n",
            "Epoch 116, gen loss=2.45927095413208,disc loss=0.2637512981891632\n",
            "Epoch 117, gen loss=3.7298996448516846,disc loss=0.36133575439453125\n",
            "Epoch 118, gen loss=2.5383119583129883,disc loss=0.34436678886413574\n",
            "Epoch 119, gen loss=1.6828436851501465,disc loss=0.7039875388145447\n",
            "Epoch 120, gen loss=2.1535823345184326,disc loss=0.800238847732544\n",
            "Epoch 121, gen loss=1.7240514755249023,disc loss=0.6655790209770203\n",
            "Epoch 122, gen loss=2.8175313472747803,disc loss=0.678037166595459\n",
            "Epoch 123, gen loss=1.8118194341659546,disc loss=0.6496660113334656\n",
            "Epoch 124, gen loss=1.9026495218276978,disc loss=0.7092523574829102\n",
            "Epoch 125, gen loss=3.00072979927063,disc loss=0.49118077754974365\n",
            "Epoch 126, gen loss=2.879042387008667,disc loss=0.29930081963539124\n",
            "Epoch 127, gen loss=3.850898504257202,disc loss=0.15942440927028656\n",
            "Epoch 128, gen loss=3.6243069171905518,disc loss=0.15157730877399445\n",
            "Epoch 129, gen loss=2.582794427871704,disc loss=0.6332926154136658\n",
            "Epoch 130, gen loss=1.801264762878418,disc loss=1.5037280321121216\n",
            "Epoch 131, gen loss=3.349410057067871,disc loss=0.8563528060913086\n",
            "Epoch 132, gen loss=2.731705665588379,disc loss=0.559751570224762\n",
            "Epoch 133, gen loss=3.5121471881866455,disc loss=0.46020278334617615\n",
            "Epoch 134, gen loss=2.749979019165039,disc loss=0.3180752694606781\n",
            "Epoch 135, gen loss=2.746570348739624,disc loss=0.5457203388214111\n",
            "Epoch 136, gen loss=2.8975002765655518,disc loss=0.7117301821708679\n",
            "Epoch 137, gen loss=2.714850664138794,disc loss=1.1688522100448608\n",
            "Epoch 138, gen loss=2.0685007572174072,disc loss=1.2871919870376587\n",
            "Epoch 139, gen loss=1.775051474571228,disc loss=0.8070259690284729\n",
            "Epoch 140, gen loss=2.107058525085449,disc loss=0.6067013144493103\n",
            "Epoch 141, gen loss=2.0589747428894043,disc loss=1.3455549478530884\n",
            "Epoch 142, gen loss=1.578590750694275,disc loss=0.7264754176139832\n",
            "Epoch 143, gen loss=1.989205241203308,disc loss=0.7517088055610657\n",
            "Epoch 144, gen loss=2.197910785675049,disc loss=0.663985550403595\n",
            "Epoch 145, gen loss=2.2275238037109375,disc loss=0.7195107936859131\n",
            "Epoch 146, gen loss=2.380589723587036,disc loss=0.8561573028564453\n",
            "Epoch 147, gen loss=1.6269668340682983,disc loss=1.2051081657409668\n",
            "Epoch 148, gen loss=1.975844383239746,disc loss=0.9491211771965027\n",
            "Epoch 149, gen loss=2.480053186416626,disc loss=0.48459553718566895\n",
            "Epoch 150, gen loss=2.1520752906799316,disc loss=0.5196747183799744\n",
            "Epoch 151, gen loss=2.537714958190918,disc loss=1.6405104398727417\n",
            "Epoch 152, gen loss=1.7331647872924805,disc loss=0.7040081024169922\n",
            "Epoch 153, gen loss=2.0596330165863037,disc loss=0.5646105408668518\n",
            "Epoch 154, gen loss=1.4396058320999146,disc loss=1.3387383222579956\n",
            "Epoch 155, gen loss=3.21008563041687,disc loss=1.6089626550674438\n",
            "Epoch 156, gen loss=3.1059255599975586,disc loss=1.0267030000686646\n",
            "Epoch 157, gen loss=3.850045919418335,disc loss=0.1497172862291336\n",
            "Epoch 158, gen loss=2.3141956329345703,disc loss=0.3502176105976105\n",
            "Epoch 159, gen loss=3.0362346172332764,disc loss=0.4691387712955475\n",
            "Epoch 160, gen loss=2.068223237991333,disc loss=0.3667001724243164\n",
            "Epoch 161, gen loss=2.271543264389038,disc loss=0.4227182865142822\n",
            "Epoch 162, gen loss=1.6724952459335327,disc loss=0.9841615557670593\n",
            "Epoch 163, gen loss=2.4300549030303955,disc loss=0.6738770604133606\n",
            "Epoch 164, gen loss=2.7707602977752686,disc loss=0.5884050726890564\n",
            "Epoch 165, gen loss=2.056666612625122,disc loss=0.38557180762290955\n",
            "Epoch 166, gen loss=2.5444209575653076,disc loss=1.1112397909164429\n",
            "Epoch 167, gen loss=2.7321345806121826,disc loss=0.5561284422874451\n",
            "Epoch 168, gen loss=2.5283961296081543,disc loss=0.3612268269062042\n",
            "Epoch 169, gen loss=2.9077835083007812,disc loss=0.30781272053718567\n",
            "Epoch 170, gen loss=2.4681050777435303,disc loss=0.38479188084602356\n",
            "Epoch 171, gen loss=2.1815361976623535,disc loss=0.5063949227333069\n",
            "Epoch 172, gen loss=2.406061887741089,disc loss=0.5232818126678467\n",
            "Epoch 173, gen loss=3.0280399322509766,disc loss=0.3317604064941406\n",
            "Epoch 174, gen loss=1.9059194326400757,disc loss=1.3203788995742798\n",
            "Epoch 175, gen loss=2.3584749698638916,disc loss=0.4225579798221588\n",
            "Epoch 176, gen loss=2.5652143955230713,disc loss=0.39110860228538513\n",
            "Epoch 177, gen loss=1.7745598554611206,disc loss=0.9347297549247742\n",
            "Epoch 178, gen loss=2.9025466442108154,disc loss=0.48889121413230896\n",
            "Epoch 179, gen loss=2.9452669620513916,disc loss=0.4412612020969391\n",
            "Epoch 180, gen loss=2.2777740955352783,disc loss=0.628346860408783\n",
            "Epoch 181, gen loss=2.4702374935150146,disc loss=0.7860958576202393\n",
            "Epoch 182, gen loss=2.656573534011841,disc loss=0.31947270035743713\n",
            "Epoch 183, gen loss=2.913278579711914,disc loss=0.28748640418052673\n",
            "Epoch 184, gen loss=2.3996434211730957,disc loss=0.5946220755577087\n",
            "Epoch 185, gen loss=3.9638264179229736,disc loss=0.8465511798858643\n",
            "Epoch 186, gen loss=2.630964517593384,disc loss=0.8971094489097595\n",
            "Epoch 187, gen loss=2.4491138458251953,disc loss=0.778204619884491\n",
            "Epoch 188, gen loss=2.240884304046631,disc loss=0.7671394348144531\n",
            "Epoch 189, gen loss=2.984300374984741,disc loss=0.8729432225227356\n",
            "Epoch 190, gen loss=2.630368709564209,disc loss=0.8694999814033508\n",
            "Epoch 191, gen loss=2.9396169185638428,disc loss=1.858442783355713\n",
            "Epoch 192, gen loss=2.13019061088562,disc loss=0.49600455164909363\n",
            "Epoch 193, gen loss=4.291440486907959,disc loss=0.3395834267139435\n",
            "Epoch 194, gen loss=2.927290201187134,disc loss=0.26540446281433105\n",
            "Epoch 195, gen loss=2.7368991374969482,disc loss=0.40681174397468567\n",
            "Epoch 196, gen loss=2.1318652629852295,disc loss=0.7924301028251648\n",
            "Epoch 197, gen loss=1.9480953216552734,disc loss=0.814736545085907\n",
            "Epoch 198, gen loss=2.1040380001068115,disc loss=0.9964017868041992\n",
            "Epoch 199, gen loss=2.1734988689422607,disc loss=0.4667440354824066\n",
            "Epoch 200, gen loss=3.5007026195526123,disc loss=0.2588448226451874\n",
            "Epoch 201, gen loss=2.283278703689575,disc loss=0.7813405990600586\n",
            "Epoch 202, gen loss=2.4619827270507812,disc loss=0.5435141921043396\n",
            "Epoch 203, gen loss=3.271373748779297,disc loss=0.33261343836784363\n",
            "Epoch 204, gen loss=2.3136842250823975,disc loss=0.41261544823646545\n",
            "Epoch 205, gen loss=2.6555166244506836,disc loss=0.8127090334892273\n",
            "Epoch 206, gen loss=1.2756768465042114,disc loss=1.5657461881637573\n",
            "Epoch 207, gen loss=2.449399709701538,disc loss=0.9452754855155945\n",
            "Epoch 208, gen loss=2.7639877796173096,disc loss=0.38547244668006897\n",
            "Epoch 209, gen loss=2.6540021896362305,disc loss=0.3337656557559967\n",
            "Epoch 210, gen loss=2.608647108078003,disc loss=0.6766705513000488\n",
            "Epoch 211, gen loss=2.154151439666748,disc loss=0.6394816040992737\n",
            "Epoch 212, gen loss=2.27947735786438,disc loss=1.0064144134521484\n",
            "Epoch 213, gen loss=2.6296303272247314,disc loss=0.7271978855133057\n",
            "Epoch 214, gen loss=2.15139102935791,disc loss=0.8086099028587341\n",
            "Epoch 215, gen loss=1.3851269483566284,disc loss=0.9417433738708496\n",
            "Epoch 216, gen loss=2.6240804195404053,disc loss=1.228258728981018\n",
            "Epoch 217, gen loss=2.696491003036499,disc loss=0.8543329238891602\n",
            "Epoch 218, gen loss=2.558344841003418,disc loss=0.7238819003105164\n",
            "Epoch 219, gen loss=3.1887996196746826,disc loss=1.3613500595092773\n",
            "Epoch 220, gen loss=2.1201865673065186,disc loss=1.62156343460083\n",
            "Epoch 221, gen loss=3.5414860248565674,disc loss=0.3485319912433624\n",
            "Epoch 222, gen loss=2.7459819316864014,disc loss=0.5967211127281189\n",
            "Epoch 223, gen loss=2.051551580429077,disc loss=0.8720059394836426\n",
            "Epoch 224, gen loss=1.931744933128357,disc loss=1.4179350137710571\n",
            "Epoch 225, gen loss=2.6802079677581787,disc loss=0.9064569473266602\n",
            "Epoch 226, gen loss=2.9439189434051514,disc loss=0.7865236401557922\n",
            "Epoch 227, gen loss=1.4720182418823242,disc loss=1.1851201057434082\n",
            "Epoch 228, gen loss=1.7350412607192993,disc loss=1.055547833442688\n",
            "Epoch 229, gen loss=1.9519635438919067,disc loss=0.9158677458763123\n",
            "Epoch 230, gen loss=1.8746901750564575,disc loss=0.7150073051452637\n",
            "Epoch 231, gen loss=2.171980381011963,disc loss=0.7839148640632629\n",
            "Epoch 232, gen loss=2.3529577255249023,disc loss=0.5837038159370422\n",
            "Epoch 233, gen loss=2.650202512741089,disc loss=0.607805609703064\n",
            "Epoch 234, gen loss=2.280844211578369,disc loss=0.39427027106285095\n",
            "Epoch 235, gen loss=1.4881919622421265,disc loss=0.8402092456817627\n",
            "Epoch 236, gen loss=2.375800848007202,disc loss=0.9905598759651184\n",
            "Epoch 237, gen loss=2.7411134243011475,disc loss=0.4764740467071533\n",
            "Epoch 238, gen loss=2.558513641357422,disc loss=0.5947451591491699\n",
            "Epoch 239, gen loss=2.1117966175079346,disc loss=1.030148983001709\n",
            "Epoch 240, gen loss=1.7736843824386597,disc loss=0.7502222061157227\n",
            "Epoch 241, gen loss=1.9319819211959839,disc loss=1.1503459215164185\n",
            "Epoch 242, gen loss=2.067277193069458,disc loss=0.8056425452232361\n",
            "Epoch 243, gen loss=2.021124839782715,disc loss=0.7699036598205566\n",
            "Epoch 244, gen loss=2.05022931098938,disc loss=0.8568128943443298\n",
            "Epoch 245, gen loss=1.861545205116272,disc loss=0.8123055100440979\n",
            "Epoch 246, gen loss=1.9578040838241577,disc loss=1.0527030229568481\n",
            "Epoch 247, gen loss=2.378744125366211,disc loss=0.7249322533607483\n",
            "Epoch 248, gen loss=2.4757518768310547,disc loss=0.47874608635902405\n",
            "Epoch 249, gen loss=2.12469482421875,disc loss=0.9164965152740479\n",
            "Epoch 250, gen loss=2.317161798477173,disc loss=0.709244966506958\n",
            "Epoch 251, gen loss=1.749437928199768,disc loss=1.0674121379852295\n",
            "Epoch 252, gen loss=2.2018673419952393,disc loss=0.5905195474624634\n",
            "Epoch 253, gen loss=2.64436411857605,disc loss=0.47115442156791687\n",
            "Epoch 254, gen loss=2.082610845565796,disc loss=0.442231148481369\n",
            "Epoch 255, gen loss=1.7206541299819946,disc loss=1.6552232503890991\n",
            "Epoch 256, gen loss=2.1276886463165283,disc loss=0.8658444285392761\n",
            "Epoch 257, gen loss=2.236192464828491,disc loss=0.6263418197631836\n",
            "Epoch 258, gen loss=1.881670355796814,disc loss=0.5252232551574707\n",
            "Epoch 259, gen loss=1.7793554067611694,disc loss=0.8808913826942444\n",
            "Epoch 260, gen loss=2.030501365661621,disc loss=0.8959822654724121\n",
            "Epoch 261, gen loss=2.0331008434295654,disc loss=0.6848526000976562\n",
            "Epoch 262, gen loss=1.9681100845336914,disc loss=0.6511690020561218\n",
            "Epoch 263, gen loss=2.1136929988861084,disc loss=0.651199460029602\n",
            "Epoch 264, gen loss=2.112852096557617,disc loss=0.7777514457702637\n",
            "Epoch 265, gen loss=2.449094533920288,disc loss=1.132997989654541\n",
            "Epoch 266, gen loss=1.8286409378051758,disc loss=0.6437327265739441\n",
            "Epoch 267, gen loss=2.383915662765503,disc loss=0.6556476354598999\n",
            "Epoch 268, gen loss=2.3609859943389893,disc loss=0.4665485620498657\n",
            "Epoch 269, gen loss=2.2049472332000732,disc loss=0.49255406856536865\n",
            "Epoch 270, gen loss=2.4425415992736816,disc loss=0.5147836804389954\n",
            "Epoch 271, gen loss=2.556023359298706,disc loss=0.4269995391368866\n",
            "Epoch 272, gen loss=2.9316108226776123,disc loss=0.2553779184818268\n",
            "Epoch 273, gen loss=3.2187299728393555,disc loss=0.31038400530815125\n",
            "Epoch 274, gen loss=3.170389413833618,disc loss=0.21220499277114868\n",
            "Epoch 275, gen loss=2.7764475345611572,disc loss=0.26597312092781067\n",
            "Epoch 276, gen loss=2.1863796710968018,disc loss=0.7024662494659424\n",
            "Epoch 277, gen loss=1.7589360475540161,disc loss=1.086482048034668\n",
            "Epoch 278, gen loss=1.8003116846084595,disc loss=0.7918503880500793\n",
            "Epoch 279, gen loss=1.7021490335464478,disc loss=1.1723415851593018\n",
            "Epoch 280, gen loss=1.9211782217025757,disc loss=0.5706332325935364\n",
            "Epoch 281, gen loss=2.9016449451446533,disc loss=0.4229534864425659\n",
            "Epoch 282, gen loss=2.7095935344696045,disc loss=0.28980931639671326\n",
            "Epoch 283, gen loss=2.8000600337982178,disc loss=0.4038315713405609\n",
            "Epoch 284, gen loss=2.033940315246582,disc loss=1.0624885559082031\n",
            "Epoch 285, gen loss=2.134920120239258,disc loss=0.5854600071907043\n",
            "Epoch 286, gen loss=2.307345390319824,disc loss=0.8424211144447327\n",
            "Epoch 287, gen loss=1.0614911317825317,disc loss=1.391133189201355\n",
            "Epoch 288, gen loss=1.8319578170776367,disc loss=1.2404263019561768\n",
            "Epoch 289, gen loss=2.115544080734253,disc loss=0.646365225315094\n",
            "Epoch 290, gen loss=1.3158105611801147,disc loss=0.8554722666740417\n",
            "Epoch 291, gen loss=2.67290997505188,disc loss=0.8581674695014954\n",
            "Epoch 292, gen loss=2.8433735370635986,disc loss=0.2774803638458252\n",
            "Epoch 293, gen loss=1.8115153312683105,disc loss=1.212514877319336\n",
            "Epoch 294, gen loss=1.5633474588394165,disc loss=0.9045235514640808\n",
            "Epoch 295, gen loss=1.840545654296875,disc loss=1.4827523231506348\n",
            "Epoch 296, gen loss=2.138787269592285,disc loss=0.577731192111969\n",
            "Epoch 297, gen loss=1.9644635915756226,disc loss=0.6923573613166809\n",
            "Epoch 298, gen loss=2.187267303466797,disc loss=0.5501757860183716\n",
            "Epoch 299, gen loss=2.1742875576019287,disc loss=0.5152599811553955\n",
            "Epoch 300, gen loss=2.1669039726257324,disc loss=0.7874040603637695\n",
            "Epoch 301, gen loss=1.8203598260879517,disc loss=0.7887506484985352\n",
            "Epoch 302, gen loss=1.7869065999984741,disc loss=1.467293381690979\n",
            "Epoch 303, gen loss=1.7821985483169556,disc loss=0.6966996192932129\n",
            "Epoch 304, gen loss=1.8825263977050781,disc loss=0.5728777647018433\n",
            "Epoch 305, gen loss=2.358793020248413,disc loss=0.5900733470916748\n",
            "Epoch 306, gen loss=2.0612261295318604,disc loss=0.5475664734840393\n",
            "Epoch 307, gen loss=2.057713270187378,disc loss=1.6370970010757446\n",
            "Epoch 308, gen loss=1.6670684814453125,disc loss=1.1427077054977417\n",
            "Epoch 309, gen loss=1.959607481956482,disc loss=0.6291141510009766\n",
            "Epoch 310, gen loss=1.9477580785751343,disc loss=0.7961463332176208\n",
            "Epoch 311, gen loss=2.3372113704681396,disc loss=0.5669252276420593\n",
            "Epoch 312, gen loss=2.42410945892334,disc loss=0.3512056767940521\n",
            "Epoch 313, gen loss=1.9388173818588257,disc loss=0.7045634388923645\n",
            "Epoch 314, gen loss=2.0437333583831787,disc loss=1.0772494077682495\n",
            "Epoch 315, gen loss=2.140091896057129,disc loss=0.4146309792995453\n",
            "Epoch 316, gen loss=2.2616875171661377,disc loss=0.4329130947589874\n",
            "Epoch 317, gen loss=1.4927420616149902,disc loss=0.9737396240234375\n",
            "Epoch 318, gen loss=2.038830041885376,disc loss=0.5449476838111877\n",
            "Epoch 319, gen loss=2.0965445041656494,disc loss=0.6415452361106873\n",
            "Epoch 320, gen loss=1.908869743347168,disc loss=0.5303654074668884\n",
            "Epoch 321, gen loss=2.589202880859375,disc loss=0.7019171118736267\n",
            "Epoch 322, gen loss=2.2600526809692383,disc loss=0.45946359634399414\n",
            "Epoch 323, gen loss=2.4364137649536133,disc loss=0.3142493665218353\n",
            "Epoch 324, gen loss=2.1931779384613037,disc loss=0.6779406666755676\n",
            "Epoch 325, gen loss=2.342315912246704,disc loss=0.5279622077941895\n",
            "Epoch 326, gen loss=2.3348803520202637,disc loss=0.4017241299152374\n",
            "Epoch 327, gen loss=2.216564178466797,disc loss=0.40301933884620667\n",
            "Epoch 328, gen loss=1.9599628448486328,disc loss=0.8406354784965515\n",
            "Epoch 329, gen loss=2.5310606956481934,disc loss=0.9336004257202148\n",
            "Epoch 330, gen loss=2.296679735183716,disc loss=0.4523988664150238\n",
            "Epoch 331, gen loss=2.2970142364501953,disc loss=0.4575921595096588\n",
            "Epoch 332, gen loss=2.8501594066619873,disc loss=0.34620293974876404\n",
            "Epoch 333, gen loss=2.2040867805480957,disc loss=0.40624871850013733\n",
            "Epoch 334, gen loss=2.4050018787384033,disc loss=0.8266386985778809\n",
            "Epoch 335, gen loss=2.5950539112091064,disc loss=0.4325392246246338\n",
            "Epoch 336, gen loss=2.9692814350128174,disc loss=0.3303595185279846\n",
            "Epoch 337, gen loss=2.4930994510650635,disc loss=0.4383624494075775\n",
            "Epoch 338, gen loss=2.2953124046325684,disc loss=0.5064124464988708\n",
            "Epoch 339, gen loss=1.5989890098571777,disc loss=0.8800523281097412\n",
            "Epoch 340, gen loss=2.222877264022827,disc loss=0.7107217311859131\n",
            "Epoch 341, gen loss=2.3109776973724365,disc loss=0.318285197019577\n",
            "Epoch 342, gen loss=2.2367095947265625,disc loss=0.5119458436965942\n",
            "Epoch 343, gen loss=2.1498022079467773,disc loss=1.16391921043396\n",
            "Epoch 344, gen loss=2.0829031467437744,disc loss=0.8598741888999939\n",
            "Epoch 345, gen loss=2.032322883605957,disc loss=0.708386242389679\n",
            "Epoch 346, gen loss=2.2378947734832764,disc loss=0.8465699553489685\n",
            "Epoch 347, gen loss=2.6581249237060547,disc loss=0.9150484204292297\n",
            "Epoch 348, gen loss=2.1641623973846436,disc loss=0.42875733971595764\n",
            "Epoch 349, gen loss=2.26694393157959,disc loss=0.44911518692970276\n",
            "Epoch 350, gen loss=2.1836740970611572,disc loss=0.6846739649772644\n",
            "Epoch 351, gen loss=2.26794171333313,disc loss=1.2389367818832397\n",
            "Epoch 352, gen loss=1.8078590631484985,disc loss=1.0613998174667358\n",
            "Epoch 353, gen loss=2.498028039932251,disc loss=0.7388579845428467\n",
            "Epoch 354, gen loss=2.0653793811798096,disc loss=0.5767447352409363\n",
            "Epoch 355, gen loss=4.005945682525635,disc loss=0.44742512702941895\n",
            "Epoch 356, gen loss=2.6586053371429443,disc loss=0.17466320097446442\n",
            "Epoch 357, gen loss=2.8923251628875732,disc loss=0.20049570500850677\n",
            "Epoch 358, gen loss=2.1880733966827393,disc loss=0.5725064873695374\n",
            "Epoch 359, gen loss=2.2466213703155518,disc loss=0.5368998050689697\n",
            "Epoch 360, gen loss=2.170374631881714,disc loss=0.7257168889045715\n",
            "Epoch 361, gen loss=2.1151092052459717,disc loss=0.48334527015686035\n",
            "Epoch 362, gen loss=2.349451780319214,disc loss=0.8944875597953796\n",
            "Epoch 363, gen loss=2.7600574493408203,disc loss=0.6005427241325378\n",
            "Epoch 364, gen loss=2.5014238357543945,disc loss=0.49770402908325195\n",
            "Epoch 365, gen loss=2.4481780529022217,disc loss=0.4112720489501953\n",
            "Epoch 366, gen loss=1.9189972877502441,disc loss=1.917836308479309\n",
            "Epoch 367, gen loss=2.4145700931549072,disc loss=1.1982251405715942\n",
            "Epoch 368, gen loss=1.9631608724594116,disc loss=0.60682213306427\n",
            "Epoch 369, gen loss=2.4191129207611084,disc loss=0.3721378743648529\n",
            "Epoch 370, gen loss=2.591404914855957,disc loss=0.6363964080810547\n",
            "Epoch 371, gen loss=2.3968002796173096,disc loss=1.0359514951705933\n",
            "Epoch 372, gen loss=2.4724082946777344,disc loss=0.757586658000946\n",
            "Epoch 373, gen loss=3.4469845294952393,disc loss=0.36101922392845154\n",
            "Epoch 374, gen loss=2.025621175765991,disc loss=1.227320671081543\n",
            "Epoch 375, gen loss=2.232046604156494,disc loss=0.6439321637153625\n",
            "Epoch 376, gen loss=2.671682357788086,disc loss=0.3042127788066864\n",
            "Epoch 377, gen loss=2.544644355773926,disc loss=0.30748510360717773\n",
            "Epoch 378, gen loss=2.450212240219116,disc loss=0.4193108081817627\n",
            "Epoch 379, gen loss=1.8689857721328735,disc loss=0.9004492163658142\n",
            "Epoch 380, gen loss=2.4581830501556396,disc loss=0.6780274510383606\n",
            "Epoch 381, gen loss=1.5483068227767944,disc loss=0.7709193825721741\n",
            "Epoch 382, gen loss=3.1504831314086914,disc loss=0.3188078701496124\n",
            "Epoch 383, gen loss=2.480768918991089,disc loss=0.7871100306510925\n",
            "Epoch 384, gen loss=2.0534675121307373,disc loss=0.5611120462417603\n",
            "Epoch 385, gen loss=2.6505110263824463,disc loss=0.45577749609947205\n",
            "Epoch 386, gen loss=2.5374956130981445,disc loss=0.41394492983818054\n",
            "Epoch 387, gen loss=1.8906798362731934,disc loss=0.8836343288421631\n",
            "Epoch 388, gen loss=1.8429962396621704,disc loss=0.823083221912384\n",
            "Epoch 389, gen loss=2.1822123527526855,disc loss=0.4924487769603729\n",
            "Epoch 390, gen loss=1.9984670877456665,disc loss=0.5286712646484375\n",
            "Epoch 391, gen loss=2.395530939102173,disc loss=0.5972380042076111\n",
            "Epoch 392, gen loss=2.226024627685547,disc loss=0.9562473297119141\n",
            "Epoch 393, gen loss=2.3886866569519043,disc loss=0.3981192409992218\n",
            "Epoch 394, gen loss=2.557504892349243,disc loss=0.43794989585876465\n",
            "Epoch 395, gen loss=2.656602144241333,disc loss=0.23589539527893066\n",
            "Epoch 396, gen loss=2.693803071975708,disc loss=0.6722478866577148\n",
            "Epoch 397, gen loss=2.6187307834625244,disc loss=0.5561484098434448\n",
            "Epoch 398, gen loss=2.2909533977508545,disc loss=0.4338187277317047\n",
            "Epoch 399, gen loss=2.8409996032714844,disc loss=0.6751673817634583\n",
            "Epoch 400, gen loss=2.24503755569458,disc loss=0.34089329838752747\n",
            "Epoch 401, gen loss=3.472379684448242,disc loss=0.2304793745279312\n",
            "Epoch 402, gen loss=2.310762405395508,disc loss=0.9183400273323059\n",
            "Epoch 403, gen loss=2.4539144039154053,disc loss=0.5531914830207825\n",
            "Epoch 404, gen loss=2.124070644378662,disc loss=0.4543629586696625\n",
            "Epoch 405, gen loss=2.7053682804107666,disc loss=0.5534990429878235\n",
            "Epoch 406, gen loss=2.463385820388794,disc loss=0.4262657165527344\n",
            "Epoch 407, gen loss=2.8139455318450928,disc loss=0.2800426483154297\n",
            "Epoch 408, gen loss=2.812122344970703,disc loss=0.9349362254142761\n",
            "Epoch 409, gen loss=2.7848899364471436,disc loss=0.7787017822265625\n",
            "Epoch 410, gen loss=2.7990291118621826,disc loss=0.1880727857351303\n",
            "Epoch 411, gen loss=3.26873779296875,disc loss=0.2373102903366089\n",
            "Epoch 412, gen loss=2.6919212341308594,disc loss=0.7043522000312805\n",
            "Epoch 413, gen loss=2.4015815258026123,disc loss=0.3285377025604248\n",
            "Epoch 414, gen loss=3.5033986568450928,disc loss=0.21755337715148926\n",
            "Epoch 415, gen loss=2.3343658447265625,disc loss=0.7015008926391602\n",
            "Epoch 416, gen loss=2.0794379711151123,disc loss=0.8150092959403992\n",
            "Epoch 417, gen loss=2.163658857345581,disc loss=0.6088094115257263\n",
            "Epoch 418, gen loss=2.3892815113067627,disc loss=0.9091205596923828\n",
            "Epoch 419, gen loss=2.2635903358459473,disc loss=0.7346643805503845\n",
            "Epoch 420, gen loss=2.579164743423462,disc loss=1.3251839876174927\n",
            "Epoch 421, gen loss=2.2416510581970215,disc loss=1.6118701696395874\n",
            "Epoch 422, gen loss=1.8440176248550415,disc loss=0.6257621645927429\n",
            "Epoch 423, gen loss=2.4886157512664795,disc loss=0.7606951594352722\n",
            "Epoch 424, gen loss=3.0607998371124268,disc loss=0.6176311373710632\n",
            "Epoch 425, gen loss=2.782543897628784,disc loss=0.23452645540237427\n",
            "Epoch 426, gen loss=2.5256519317626953,disc loss=0.3445545732975006\n",
            "Epoch 427, gen loss=2.606644630432129,disc loss=0.4436074197292328\n",
            "Epoch 428, gen loss=2.5897998809814453,disc loss=0.3868044912815094\n",
            "Epoch 429, gen loss=2.289520025253296,disc loss=0.8786578178405762\n",
            "Epoch 430, gen loss=2.3538496494293213,disc loss=0.3957119882106781\n",
            "Epoch 431, gen loss=2.5764708518981934,disc loss=0.7776740193367004\n",
            "Epoch 432, gen loss=2.384896993637085,disc loss=0.4053226411342621\n",
            "Epoch 433, gen loss=2.7582004070281982,disc loss=0.284322053194046\n",
            "Epoch 434, gen loss=2.2462270259857178,disc loss=0.5024569034576416\n",
            "Epoch 435, gen loss=3.2449123859405518,disc loss=0.32176801562309265\n",
            "Epoch 436, gen loss=3.044283628463745,disc loss=0.2636534869670868\n",
            "Epoch 437, gen loss=2.5461606979370117,disc loss=0.374672532081604\n",
            "Epoch 438, gen loss=3.031097173690796,disc loss=0.7540347576141357\n",
            "Epoch 439, gen loss=2.9157750606536865,disc loss=0.729147732257843\n",
            "Epoch 440, gen loss=2.2025372982025146,disc loss=0.5325781106948853\n",
            "Epoch 441, gen loss=2.022442579269409,disc loss=0.5766506195068359\n",
            "Epoch 442, gen loss=2.260709047317505,disc loss=0.5136256217956543\n",
            "Epoch 443, gen loss=2.6929893493652344,disc loss=0.4845522344112396\n",
            "Epoch 444, gen loss=3.265700340270996,disc loss=0.32301095128059387\n",
            "Epoch 445, gen loss=2.8771324157714844,disc loss=0.47107434272766113\n",
            "Epoch 446, gen loss=3.1392593383789062,disc loss=0.4643450975418091\n",
            "Epoch 447, gen loss=3.4855453968048096,disc loss=0.230011984705925\n",
            "Epoch 448, gen loss=3.0611488819122314,disc loss=0.22627347707748413\n",
            "Epoch 449, gen loss=2.9414989948272705,disc loss=0.45185303688049316\n",
            "Epoch 450, gen loss=2.192462921142578,disc loss=0.4185374975204468\n",
            "Epoch 451, gen loss=2.4669063091278076,disc loss=0.6381774544715881\n",
            "Epoch 452, gen loss=2.2558441162109375,disc loss=0.4030689299106598\n",
            "Epoch 453, gen loss=2.7505226135253906,disc loss=0.3143457770347595\n",
            "Epoch 454, gen loss=2.916261672973633,disc loss=0.3975349962711334\n",
            "Epoch 455, gen loss=3.1467502117156982,disc loss=0.40051570534706116\n",
            "Epoch 456, gen loss=3.1429970264434814,disc loss=0.3224354684352875\n",
            "Epoch 457, gen loss=2.6824886798858643,disc loss=0.49509677290916443\n",
            "Epoch 458, gen loss=2.3879575729370117,disc loss=0.6667666435241699\n",
            "Epoch 459, gen loss=2.796241044998169,disc loss=1.3326047658920288\n",
            "Epoch 460, gen loss=2.726093292236328,disc loss=0.8478546142578125\n",
            "Epoch 461, gen loss=3.0524206161499023,disc loss=1.495691180229187\n",
            "Epoch 462, gen loss=2.460651159286499,disc loss=1.2302982807159424\n",
            "Epoch 463, gen loss=2.26081919670105,disc loss=0.5138072967529297\n",
            "Epoch 464, gen loss=2.5693724155426025,disc loss=0.9154147505760193\n",
            "Epoch 465, gen loss=2.679145097732544,disc loss=0.93541020154953\n",
            "Epoch 466, gen loss=2.1316423416137695,disc loss=0.801929235458374\n",
            "Epoch 467, gen loss=3.0456268787384033,disc loss=0.3330937325954437\n",
            "Epoch 468, gen loss=2.825976610183716,disc loss=0.7488927841186523\n",
            "Epoch 469, gen loss=2.205517530441284,disc loss=0.6673929691314697\n",
            "Epoch 470, gen loss=2.211271047592163,disc loss=0.9632673859596252\n",
            "Epoch 471, gen loss=2.7070157527923584,disc loss=0.605208694934845\n",
            "Epoch 472, gen loss=3.0476248264312744,disc loss=0.8856537938117981\n",
            "Epoch 473, gen loss=2.5749902725219727,disc loss=0.46209827065467834\n",
            "Epoch 474, gen loss=2.410402297973633,disc loss=0.4557548463344574\n",
            "Epoch 475, gen loss=2.3843653202056885,disc loss=0.397165447473526\n",
            "Epoch 476, gen loss=2.272578239440918,disc loss=0.40210604667663574\n",
            "Epoch 477, gen loss=2.4975807666778564,disc loss=0.7814201712608337\n",
            "Epoch 478, gen loss=2.4472124576568604,disc loss=0.5208309888839722\n",
            "Epoch 479, gen loss=2.877974271774292,disc loss=0.4782586991786957\n",
            "Epoch 480, gen loss=2.4261910915374756,disc loss=0.8562862277030945\n",
            "Epoch 481, gen loss=2.078848123550415,disc loss=0.8961668610572815\n",
            "Epoch 482, gen loss=1.9267581701278687,disc loss=0.5783848762512207\n",
            "Epoch 483, gen loss=3.095820188522339,disc loss=0.3363892734050751\n",
            "Epoch 484, gen loss=2.0482444763183594,disc loss=0.4476427733898163\n",
            "Epoch 485, gen loss=2.658818006515503,disc loss=1.0021628141403198\n",
            "Epoch 486, gen loss=2.9522340297698975,disc loss=0.6240604519844055\n",
            "Epoch 487, gen loss=2.722414016723633,disc loss=0.32180067896842957\n",
            "Epoch 488, gen loss=2.84393572807312,disc loss=0.4029359817504883\n",
            "Epoch 489, gen loss=2.6822397708892822,disc loss=0.4479626417160034\n",
            "Epoch 490, gen loss=2.983567476272583,disc loss=0.22137703001499176\n",
            "Epoch 491, gen loss=2.6588220596313477,disc loss=0.41786155104637146\n",
            "Epoch 492, gen loss=2.2613604068756104,disc loss=0.4017593562602997\n",
            "Epoch 493, gen loss=1.9934653043746948,disc loss=0.680494487285614\n",
            "Epoch 494, gen loss=2.5048253536224365,disc loss=0.4837358891963959\n",
            "Epoch 495, gen loss=2.5343942642211914,disc loss=0.8361845016479492\n",
            "Epoch 496, gen loss=2.9815824031829834,disc loss=0.7588443756103516\n",
            "Epoch 497, gen loss=2.4477977752685547,disc loss=0.4583238661289215\n",
            "Epoch 498, gen loss=3.3185975551605225,disc loss=0.29113367199897766\n",
            "Epoch 499, gen loss=2.768733263015747,disc loss=0.7685568928718567\n",
            "Epoch 500, gen loss=2.726639747619629,disc loss=0.4283044636249542\n",
            "Epoch 501, gen loss=3.2569057941436768,disc loss=0.229889914393425\n",
            "Epoch 502, gen loss=3.3457190990448,disc loss=0.37065017223358154\n",
            "Epoch 503, gen loss=2.4760677814483643,disc loss=0.35896036028862\n",
            "Epoch 504, gen loss=2.495471715927124,disc loss=0.47665509581565857\n",
            "Epoch 505, gen loss=2.894097089767456,disc loss=0.5292940735816956\n",
            "Epoch 506, gen loss=2.772954225540161,disc loss=0.7991370558738708\n",
            "Epoch 507, gen loss=2.464545488357544,disc loss=1.3085051774978638\n",
            "Epoch 508, gen loss=2.619952440261841,disc loss=1.0169252157211304\n",
            "Epoch 509, gen loss=3.467643976211548,disc loss=0.34355196356773376\n",
            "Epoch 510, gen loss=2.5077199935913086,disc loss=0.504098653793335\n",
            "Epoch 511, gen loss=2.6203300952911377,disc loss=0.34208187460899353\n",
            "Epoch 512, gen loss=2.759443998336792,disc loss=0.4688918888568878\n",
            "Epoch 513, gen loss=2.512587308883667,disc loss=0.40755295753479004\n",
            "Epoch 514, gen loss=2.566462755203247,disc loss=0.36796799302101135\n",
            "Epoch 515, gen loss=2.9721202850341797,disc loss=0.6952393054962158\n",
            "Epoch 516, gen loss=3.0937328338623047,disc loss=0.4262024462223053\n",
            "Epoch 517, gen loss=3.442197799682617,disc loss=0.21889829635620117\n",
            "Epoch 518, gen loss=2.5146090984344482,disc loss=0.8989675045013428\n",
            "Epoch 519, gen loss=2.133183717727661,disc loss=0.4968993663787842\n",
            "Epoch 520, gen loss=2.417271852493286,disc loss=0.6751163005828857\n",
            "Epoch 521, gen loss=2.785655975341797,disc loss=0.2681615948677063\n",
            "Epoch 522, gen loss=2.5042827129364014,disc loss=0.29661163687705994\n",
            "Epoch 523, gen loss=3.356429100036621,disc loss=0.32371267676353455\n",
            "Epoch 524, gen loss=2.9185943603515625,disc loss=0.2031848281621933\n",
            "Epoch 525, gen loss=3.1687171459198,disc loss=0.43124744296073914\n",
            "Epoch 526, gen loss=2.853264808654785,disc loss=0.3425652086734772\n",
            "Epoch 527, gen loss=3.1555168628692627,disc loss=0.16665640473365784\n",
            "Epoch 528, gen loss=3.818052291870117,disc loss=0.1876516342163086\n",
            "Epoch 529, gen loss=3.0587854385375977,disc loss=0.26044154167175293\n",
            "Epoch 530, gen loss=3.8695952892303467,disc loss=0.14596585929393768\n",
            "Epoch 531, gen loss=3.2730329036712646,disc loss=0.13282325863838196\n",
            "Epoch 532, gen loss=2.366166830062866,disc loss=0.4264000952243805\n",
            "Epoch 533, gen loss=3.0003082752227783,disc loss=0.9920249581336975\n",
            "Epoch 534, gen loss=3.977093458175659,disc loss=0.5881167054176331\n",
            "Epoch 535, gen loss=2.9940919876098633,disc loss=0.21085841953754425\n",
            "Epoch 536, gen loss=3.312262773513794,disc loss=0.588266909122467\n",
            "Epoch 537, gen loss=3.2463366985321045,disc loss=0.3570365905761719\n",
            "Epoch 538, gen loss=2.8224306106567383,disc loss=0.375872939825058\n",
            "Epoch 539, gen loss=3.4385128021240234,disc loss=0.3385263681411743\n",
            "Epoch 540, gen loss=3.205993413925171,disc loss=0.13443194329738617\n",
            "Epoch 541, gen loss=2.6815614700317383,disc loss=0.25958573818206787\n",
            "Epoch 542, gen loss=2.614842414855957,disc loss=0.6714501976966858\n",
            "Epoch 543, gen loss=3.213306427001953,disc loss=0.43357741832733154\n",
            "Epoch 544, gen loss=2.7078325748443604,disc loss=0.38469985127449036\n",
            "Epoch 545, gen loss=2.535515069961548,disc loss=0.35200127959251404\n",
            "Epoch 546, gen loss=2.614708662033081,disc loss=1.3352826833724976\n",
            "Epoch 547, gen loss=5.146908283233643,disc loss=1.6745330095291138\n",
            "Epoch 548, gen loss=2.940814733505249,disc loss=0.24225597083568573\n",
            "Epoch 549, gen loss=3.2939088344573975,disc loss=0.21485762298107147\n",
            "Epoch 550, gen loss=2.5052640438079834,disc loss=0.338364839553833\n",
            "Epoch 551, gen loss=2.705364942550659,disc loss=0.49663281440734863\n",
            "Epoch 552, gen loss=3.3131625652313232,disc loss=0.6634466052055359\n",
            "Epoch 553, gen loss=4.599930286407471,disc loss=2.027158498764038\n",
            "Epoch 554, gen loss=1.8764677047729492,disc loss=0.6881596446037292\n",
            "Epoch 555, gen loss=3.8544862270355225,disc loss=0.2120552808046341\n",
            "Epoch 556, gen loss=2.493730306625366,disc loss=1.6050676107406616\n",
            "Epoch 557, gen loss=2.791309118270874,disc loss=1.1314743757247925\n",
            "Epoch 558, gen loss=3.9624998569488525,disc loss=1.507411003112793\n",
            "Epoch 559, gen loss=3.0073862075805664,disc loss=0.5924794673919678\n",
            "Epoch 560, gen loss=3.602951765060425,disc loss=0.36332449316978455\n",
            "Epoch 561, gen loss=2.4343173503875732,disc loss=0.46300339698791504\n",
            "Epoch 562, gen loss=2.5858733654022217,disc loss=0.4238504469394684\n",
            "Epoch 563, gen loss=2.1996712684631348,disc loss=0.7702019214630127\n",
            "Epoch 564, gen loss=2.1499950885772705,disc loss=0.43831491470336914\n",
            "Epoch 565, gen loss=2.6464951038360596,disc loss=0.6803727149963379\n",
            "Epoch 566, gen loss=2.3878800868988037,disc loss=0.369255930185318\n",
            "Epoch 567, gen loss=2.862531900405884,disc loss=0.2874021828174591\n",
            "Epoch 568, gen loss=2.8966777324676514,disc loss=0.2800670564174652\n",
            "Epoch 569, gen loss=3.0600669384002686,disc loss=0.4993368685245514\n",
            "Epoch 570, gen loss=2.4363534450531006,disc loss=0.37937068939208984\n",
            "Epoch 571, gen loss=2.501582384109497,disc loss=0.4021552503108978\n",
            "Epoch 572, gen loss=2.7827417850494385,disc loss=0.5680943131446838\n",
            "Epoch 573, gen loss=1.998528003692627,disc loss=0.29719096422195435\n",
            "Epoch 574, gen loss=3.4458301067352295,disc loss=0.3097170889377594\n",
            "Epoch 575, gen loss=2.2247955799102783,disc loss=0.2966841161251068\n",
            "Epoch 576, gen loss=3.2555153369903564,disc loss=0.22853024303913116\n",
            "Epoch 577, gen loss=3.207038164138794,disc loss=0.24809320271015167\n",
            "Epoch 578, gen loss=2.523069143295288,disc loss=0.290085107088089\n",
            "Epoch 579, gen loss=3.097616195678711,disc loss=0.27380940318107605\n",
            "Epoch 580, gen loss=3.3725385665893555,disc loss=0.21343255043029785\n",
            "Epoch 581, gen loss=3.240248680114746,disc loss=0.2692943513393402\n",
            "Epoch 582, gen loss=2.9367306232452393,disc loss=0.2613912522792816\n",
            "Epoch 583, gen loss=2.3052127361297607,disc loss=0.6104459166526794\n",
            "Epoch 584, gen loss=2.7708852291107178,disc loss=0.8007394671440125\n",
            "Epoch 585, gen loss=2.6176340579986572,disc loss=0.32563725113868713\n",
            "Epoch 586, gen loss=2.3755834102630615,disc loss=0.4792815148830414\n",
            "Epoch 587, gen loss=3.1134204864501953,disc loss=0.43333545327186584\n",
            "Epoch 588, gen loss=2.7022511959075928,disc loss=0.37712135910987854\n",
            "Epoch 589, gen loss=3.3203418254852295,disc loss=0.2841343581676483\n",
            "Epoch 590, gen loss=3.594280958175659,disc loss=0.460653692483902\n",
            "Epoch 591, gen loss=2.8227975368499756,disc loss=0.34138381481170654\n",
            "Epoch 592, gen loss=2.6676013469696045,disc loss=0.46608641743659973\n",
            "Epoch 593, gen loss=2.8467769622802734,disc loss=0.4830296039581299\n",
            "Epoch 594, gen loss=2.5904319286346436,disc loss=0.8613149523735046\n",
            "Epoch 595, gen loss=2.8252620697021484,disc loss=0.580064594745636\n",
            "Epoch 596, gen loss=2.5329344272613525,disc loss=0.37950965762138367\n",
            "Epoch 597, gen loss=3.2914788722991943,disc loss=0.251413494348526\n",
            "Epoch 598, gen loss=4.074710845947266,disc loss=0.14238250255584717\n",
            "Epoch 599, gen loss=3.7486822605133057,disc loss=0.3023308217525482\n",
            "Epoch 600, gen loss=3.752687454223633,disc loss=0.9041668772697449\n",
            "Epoch 601, gen loss=2.2733452320098877,disc loss=0.44597530364990234\n",
            "Epoch 602, gen loss=3.0033037662506104,disc loss=0.36286982893943787\n",
            "Epoch 603, gen loss=2.616471290588379,disc loss=0.6631229519844055\n",
            "Epoch 604, gen loss=3.115692138671875,disc loss=0.6118795275688171\n",
            "Epoch 605, gen loss=2.8832666873931885,disc loss=0.6862707734107971\n",
            "Epoch 606, gen loss=2.8321635723114014,disc loss=0.43243470788002014\n",
            "Epoch 607, gen loss=2.7534759044647217,disc loss=0.390423983335495\n",
            "Epoch 608, gen loss=2.9631128311157227,disc loss=0.5959744453430176\n",
            "Epoch 609, gen loss=3.325838088989258,disc loss=0.7462113499641418\n",
            "Epoch 610, gen loss=2.464977741241455,disc loss=0.34597495198249817\n",
            "Epoch 611, gen loss=3.265199661254883,disc loss=0.48836028575897217\n",
            "Epoch 612, gen loss=3.392319917678833,disc loss=0.5218355655670166\n",
            "Epoch 613, gen loss=3.3267714977264404,disc loss=0.5385987758636475\n",
            "Epoch 614, gen loss=3.918060064315796,disc loss=0.28540360927581787\n",
            "Epoch 615, gen loss=2.8520610332489014,disc loss=0.279412180185318\n",
            "Epoch 616, gen loss=2.6052663326263428,disc loss=0.3987829387187958\n",
            "Epoch 617, gen loss=2.726018190383911,disc loss=0.4532771408557892\n",
            "Epoch 618, gen loss=2.563265562057495,disc loss=0.4676819145679474\n",
            "Epoch 619, gen loss=2.4389050006866455,disc loss=0.2665988802909851\n",
            "Epoch 620, gen loss=3.1312828063964844,disc loss=0.4024653136730194\n",
            "Epoch 621, gen loss=3.0599372386932373,disc loss=0.2955490052700043\n",
            "Epoch 622, gen loss=3.1589794158935547,disc loss=0.5327368378639221\n",
            "Epoch 623, gen loss=3.610905408859253,disc loss=0.3444501459598541\n",
            "Epoch 624, gen loss=2.9079363346099854,disc loss=0.18998496234416962\n",
            "Epoch 625, gen loss=3.1370837688446045,disc loss=0.30620327591896057\n",
            "Epoch 626, gen loss=2.8307712078094482,disc loss=0.39905229210853577\n",
            "Epoch 627, gen loss=2.905358076095581,disc loss=0.2855380177497864\n",
            "Epoch 628, gen loss=2.940222978591919,disc loss=0.5607345104217529\n",
            "Epoch 629, gen loss=2.695232391357422,disc loss=0.30484506487846375\n",
            "Epoch 630, gen loss=3.0113608837127686,disc loss=0.439304918050766\n",
            "Epoch 631, gen loss=2.454854726791382,disc loss=0.5539210438728333\n",
            "Epoch 632, gen loss=2.6902523040771484,disc loss=0.26490363478660583\n",
            "Epoch 633, gen loss=2.961043357849121,disc loss=0.29864180088043213\n",
            "Epoch 634, gen loss=3.0787084102630615,disc loss=0.28893303871154785\n",
            "Epoch 635, gen loss=2.7746551036834717,disc loss=0.2169843465089798\n",
            "Epoch 636, gen loss=3.0171563625335693,disc loss=0.29061052203178406\n",
            "Epoch 637, gen loss=3.0706870555877686,disc loss=0.25066956877708435\n",
            "Epoch 638, gen loss=3.402996778488159,disc loss=0.25760239362716675\n",
            "Epoch 639, gen loss=2.9014158248901367,disc loss=0.18515723943710327\n",
            "Epoch 640, gen loss=3.626310348510742,disc loss=0.6488552093505859\n",
            "Epoch 641, gen loss=3.041890859603882,disc loss=0.1580313891172409\n",
            "Epoch 642, gen loss=3.654764413833618,disc loss=0.13478998839855194\n",
            "Epoch 643, gen loss=3.3570353984832764,disc loss=0.23680591583251953\n",
            "Epoch 644, gen loss=3.0291337966918945,disc loss=0.41835710406303406\n",
            "Epoch 645, gen loss=3.9114272594451904,disc loss=0.3586064279079437\n",
            "Epoch 646, gen loss=3.3140928745269775,disc loss=0.2848872244358063\n",
            "Epoch 647, gen loss=2.9755618572235107,disc loss=0.754360020160675\n",
            "Epoch 648, gen loss=3.8046348094940186,disc loss=0.44904863834381104\n",
            "Epoch 649, gen loss=2.883699417114258,disc loss=0.3022973835468292\n",
            "Epoch 650, gen loss=2.479782819747925,disc loss=0.39681968092918396\n",
            "Epoch 651, gen loss=2.317660331726074,disc loss=0.5247790217399597\n",
            "Epoch 652, gen loss=3.356297731399536,disc loss=0.5397221446037292\n",
            "Epoch 653, gen loss=3.320903778076172,disc loss=0.7629351615905762\n",
            "Epoch 654, gen loss=2.727715253829956,disc loss=0.23484544456005096\n",
            "Epoch 655, gen loss=3.2365477085113525,disc loss=0.20622092485427856\n",
            "Epoch 656, gen loss=3.3962314128875732,disc loss=0.35234537720680237\n",
            "Epoch 657, gen loss=3.1303184032440186,disc loss=0.3090735971927643\n",
            "Epoch 658, gen loss=3.2532870769500732,disc loss=0.19475378096103668\n",
            "Epoch 659, gen loss=3.2324066162109375,disc loss=0.40942540764808655\n",
            "Epoch 660, gen loss=3.0144612789154053,disc loss=0.212929368019104\n",
            "Epoch 661, gen loss=3.0719311237335205,disc loss=0.5640053749084473\n",
            "Epoch 662, gen loss=2.8828325271606445,disc loss=0.2996334135532379\n",
            "Epoch 663, gen loss=3.383582353591919,disc loss=0.21348179876804352\n",
            "Epoch 664, gen loss=3.806424856185913,disc loss=0.11890079826116562\n",
            "Epoch 665, gen loss=3.7228307723999023,disc loss=0.27559491991996765\n",
            "Epoch 666, gen loss=3.428129196166992,disc loss=0.18370258808135986\n",
            "Epoch 667, gen loss=3.6172497272491455,disc loss=0.216589093208313\n",
            "Epoch 668, gen loss=3.292562246322632,disc loss=0.3722258508205414\n",
            "Epoch 669, gen loss=3.917787551879883,disc loss=0.4898407459259033\n",
            "Epoch 670, gen loss=2.567146062850952,disc loss=0.6081859469413757\n",
            "Epoch 671, gen loss=3.817445755004883,disc loss=0.5333018898963928\n",
            "Epoch 672, gen loss=5.505849361419678,disc loss=0.6147850155830383\n",
            "Epoch 673, gen loss=3.158108949661255,disc loss=0.4410684108734131\n",
            "Epoch 674, gen loss=3.56069278717041,disc loss=0.26688554883003235\n",
            "Epoch 675, gen loss=2.9320194721221924,disc loss=0.21089084446430206\n",
            "Epoch 676, gen loss=3.0950257778167725,disc loss=0.2903907597064972\n",
            "Epoch 677, gen loss=2.1553127765655518,disc loss=0.555923581123352\n",
            "Epoch 678, gen loss=3.2018489837646484,disc loss=0.27979621291160583\n",
            "Epoch 679, gen loss=2.7105729579925537,disc loss=0.3780708312988281\n",
            "Epoch 680, gen loss=2.9307940006256104,disc loss=0.4483450949192047\n",
            "Epoch 681, gen loss=3.2301084995269775,disc loss=0.2946936786174774\n",
            "Epoch 682, gen loss=3.252268075942993,disc loss=0.2106020599603653\n",
            "Epoch 683, gen loss=3.3103673458099365,disc loss=1.4217087030410767\n",
            "Epoch 684, gen loss=4.260225772857666,disc loss=1.3529921770095825\n",
            "Epoch 685, gen loss=3.626117706298828,disc loss=0.3235015571117401\n",
            "Epoch 686, gen loss=2.780158042907715,disc loss=0.27801844477653503\n",
            "Epoch 687, gen loss=3.4613773822784424,disc loss=0.3854266107082367\n",
            "Epoch 688, gen loss=2.8214991092681885,disc loss=0.46903476119041443\n",
            "Epoch 689, gen loss=2.8596560955047607,disc loss=0.30816856026649475\n",
            "Epoch 690, gen loss=2.9450931549072266,disc loss=0.4186150133609772\n",
            "Epoch 691, gen loss=3.0241873264312744,disc loss=0.2656443119049072\n",
            "Epoch 692, gen loss=3.5383412837982178,disc loss=0.29192447662353516\n",
            "Epoch 693, gen loss=3.2042160034179688,disc loss=0.40669289231300354\n",
            "Epoch 694, gen loss=3.793102979660034,disc loss=0.709629237651825\n",
            "Epoch 695, gen loss=2.821828603744507,disc loss=0.2879510521888733\n",
            "Epoch 696, gen loss=4.056491374969482,disc loss=0.12290921062231064\n",
            "Epoch 697, gen loss=3.1008188724517822,disc loss=0.2734887897968292\n",
            "Epoch 698, gen loss=3.314547300338745,disc loss=0.253143310546875\n",
            "Epoch 699, gen loss=3.3666722774505615,disc loss=0.2830340564250946\n",
            "Epoch 700, gen loss=3.1529152393341064,disc loss=0.8813900947570801\n",
            "Epoch 701, gen loss=4.577093601226807,disc loss=1.4385371208190918\n",
            "Epoch 702, gen loss=3.153780698776245,disc loss=0.18245364725589752\n",
            "Epoch 703, gen loss=2.9383041858673096,disc loss=0.19120727479457855\n",
            "Epoch 704, gen loss=2.863858461380005,disc loss=0.9213013052940369\n",
            "Epoch 705, gen loss=3.944075345993042,disc loss=0.21119922399520874\n",
            "Epoch 706, gen loss=2.9722506999969482,disc loss=0.334060400724411\n",
            "Epoch 707, gen loss=3.613692283630371,disc loss=0.37649092078208923\n",
            "Epoch 708, gen loss=2.7011024951934814,disc loss=0.24853800237178802\n",
            "Epoch 709, gen loss=2.780118703842163,disc loss=0.5771748423576355\n",
            "Epoch 710, gen loss=2.596912145614624,disc loss=0.2120303064584732\n",
            "Epoch 711, gen loss=3.0251171588897705,disc loss=0.4778430461883545\n",
            "Epoch 712, gen loss=2.8968465328216553,disc loss=0.5504190325737\n",
            "Epoch 713, gen loss=3.374659538269043,disc loss=0.24492304027080536\n",
            "Epoch 714, gen loss=3.4409191608428955,disc loss=0.30242276191711426\n",
            "Epoch 715, gen loss=3.210268020629883,disc loss=0.31402260065078735\n",
            "Epoch 716, gen loss=4.584173202514648,disc loss=0.7962457537651062\n",
            "Epoch 717, gen loss=5.136861324310303,disc loss=1.079997181892395\n",
            "Epoch 718, gen loss=4.274774074554443,disc loss=0.1877509504556656\n",
            "Epoch 719, gen loss=2.901655435562134,disc loss=0.3249342739582062\n",
            "Epoch 720, gen loss=3.1351191997528076,disc loss=0.552969217300415\n",
            "Epoch 721, gen loss=2.554457902908325,disc loss=0.6390599608421326\n",
            "Epoch 722, gen loss=2.9031741619110107,disc loss=0.5719783902168274\n",
            "Epoch 723, gen loss=2.7924728393554688,disc loss=0.6542413830757141\n",
            "Epoch 724, gen loss=3.4594790935516357,disc loss=0.7495782971382141\n",
            "Epoch 725, gen loss=2.8060905933380127,disc loss=0.2507767975330353\n",
            "Epoch 726, gen loss=3.875668525695801,disc loss=0.1199214830994606\n",
            "Epoch 727, gen loss=3.123188018798828,disc loss=0.6584276556968689\n",
            "Epoch 728, gen loss=2.9012439250946045,disc loss=0.2288786619901657\n",
            "Epoch 729, gen loss=4.219151973724365,disc loss=0.07589486986398697\n",
            "Epoch 730, gen loss=3.5981318950653076,disc loss=0.1832021027803421\n",
            "Epoch 731, gen loss=2.6045780181884766,disc loss=0.33285900950431824\n",
            "Epoch 732, gen loss=3.3651516437530518,disc loss=0.3627989590167999\n",
            "Epoch 733, gen loss=3.3360595703125,disc loss=0.20912015438079834\n",
            "Epoch 734, gen loss=3.508141279220581,disc loss=0.28169354796409607\n",
            "Epoch 735, gen loss=3.7249209880828857,disc loss=0.7041537165641785\n",
            "Epoch 736, gen loss=3.8287601470947266,disc loss=1.013490915298462\n",
            "Epoch 737, gen loss=2.463900566101074,disc loss=0.2470589429140091\n",
            "Epoch 738, gen loss=3.529355764389038,disc loss=0.28603965044021606\n",
            "Epoch 739, gen loss=2.744252920150757,disc loss=0.24872280657291412\n",
            "Epoch 740, gen loss=3.4079430103302,disc loss=0.45977210998535156\n",
            "Epoch 741, gen loss=3.1952250003814697,disc loss=0.460161954164505\n",
            "Epoch 742, gen loss=3.5885841846466064,disc loss=0.2404128909111023\n",
            "Epoch 743, gen loss=3.128880262374878,disc loss=0.17856638133525848\n",
            "Epoch 744, gen loss=3.45499324798584,disc loss=0.20753520727157593\n",
            "Epoch 745, gen loss=3.4312307834625244,disc loss=0.25768253207206726\n",
            "Epoch 746, gen loss=3.1727206707000732,disc loss=0.32775580883026123\n",
            "Epoch 747, gen loss=3.2166411876678467,disc loss=0.1812909096479416\n",
            "Epoch 748, gen loss=3.4699294567108154,disc loss=0.1427050679922104\n",
            "Epoch 749, gen loss=2.9828574657440186,disc loss=0.335295170545578\n",
            "Epoch 750, gen loss=4.585532188415527,disc loss=0.31322941184043884\n",
            "Epoch 751, gen loss=4.043542861938477,disc loss=0.3887213170528412\n",
            "Epoch 752, gen loss=3.550156593322754,disc loss=0.5847751498222351\n",
            "Epoch 753, gen loss=4.221072673797607,disc loss=0.3723674714565277\n",
            "Epoch 754, gen loss=3.469837188720703,disc loss=0.22002601623535156\n",
            "Epoch 755, gen loss=3.8080508708953857,disc loss=0.15784157812595367\n",
            "Epoch 756, gen loss=3.4717166423797607,disc loss=0.18283064663410187\n",
            "Epoch 757, gen loss=3.774428606033325,disc loss=0.5068500638008118\n",
            "Epoch 758, gen loss=2.73063063621521,disc loss=0.3045669496059418\n",
            "Epoch 759, gen loss=3.143023729324341,disc loss=0.6082296371459961\n",
            "Epoch 760, gen loss=4.196900844573975,disc loss=0.24246865510940552\n",
            "Epoch 761, gen loss=3.676098585128784,disc loss=0.23777277767658234\n",
            "Epoch 762, gen loss=3.357346773147583,disc loss=0.446002334356308\n",
            "Epoch 763, gen loss=3.6231658458709717,disc loss=0.17074190080165863\n",
            "Epoch 764, gen loss=3.4445769786834717,disc loss=0.21737420558929443\n",
            "Epoch 765, gen loss=3.523416519165039,disc loss=0.20694921910762787\n",
            "Epoch 766, gen loss=3.005843162536621,disc loss=0.3184830844402313\n",
            "Epoch 767, gen loss=2.780759811401367,disc loss=0.20646065473556519\n",
            "Epoch 768, gen loss=4.615714073181152,disc loss=0.16476155817508698\n",
            "Epoch 769, gen loss=3.422212600708008,disc loss=1.1301134824752808\n",
            "Epoch 770, gen loss=4.286377429962158,disc loss=0.6155121326446533\n",
            "Epoch 771, gen loss=4.766562461853027,disc loss=0.24493445456027985\n",
            "Epoch 772, gen loss=3.895669937133789,disc loss=0.11276710778474808\n",
            "Epoch 773, gen loss=3.488792657852173,disc loss=0.17672012746334076\n",
            "Epoch 774, gen loss=3.3514416217803955,disc loss=0.19322961568832397\n",
            "Epoch 775, gen loss=2.989590883255005,disc loss=0.2084253579378128\n",
            "Epoch 776, gen loss=3.354491949081421,disc loss=0.43391868472099304\n",
            "Epoch 777, gen loss=3.4459025859832764,disc loss=0.2114267796278\n",
            "Epoch 778, gen loss=3.620043992996216,disc loss=0.16978603601455688\n",
            "Epoch 779, gen loss=3.0089128017425537,disc loss=0.26563069224357605\n",
            "Epoch 780, gen loss=3.0993003845214844,disc loss=0.6159704923629761\n",
            "Epoch 781, gen loss=4.4711079597473145,disc loss=0.46558716893196106\n",
            "Epoch 782, gen loss=3.3412845134735107,disc loss=0.3055362403392792\n",
            "Epoch 783, gen loss=3.5231494903564453,disc loss=0.45115411281585693\n",
            "Epoch 784, gen loss=3.497235059738159,disc loss=0.11225850135087967\n",
            "Epoch 785, gen loss=3.7022998332977295,disc loss=0.1565622240304947\n",
            "Epoch 786, gen loss=3.606473207473755,disc loss=0.30016061663627625\n",
            "Epoch 787, gen loss=3.771491765975952,disc loss=0.6434252262115479\n",
            "Epoch 788, gen loss=4.312541484832764,disc loss=0.536963939666748\n",
            "Epoch 789, gen loss=3.2256929874420166,disc loss=0.45186343789100647\n",
            "Epoch 790, gen loss=3.8515758514404297,disc loss=0.13165581226348877\n",
            "Epoch 791, gen loss=3.9333722591400146,disc loss=0.13827510178089142\n",
            "Epoch 792, gen loss=3.9864444732666016,disc loss=0.15094076097011566\n",
            "Epoch 793, gen loss=3.8671066761016846,disc loss=0.356420636177063\n",
            "Epoch 794, gen loss=3.031216621398926,disc loss=0.20374423265457153\n",
            "Epoch 795, gen loss=3.5518369674682617,disc loss=0.21428973972797394\n",
            "Epoch 796, gen loss=2.4975101947784424,disc loss=0.6089019179344177\n",
            "Epoch 797, gen loss=3.3988049030303955,disc loss=0.10499929636716843\n",
            "Epoch 798, gen loss=3.1499459743499756,disc loss=0.15060658752918243\n",
            "Epoch 799, gen loss=3.939969062805176,disc loss=0.13825416564941406\n",
            "Epoch 800, gen loss=3.5697238445281982,disc loss=0.17527413368225098\n",
            "Epoch 801, gen loss=3.9173152446746826,disc loss=0.22491098940372467\n",
            "Epoch 802, gen loss=3.081157922744751,disc loss=0.3338332176208496\n",
            "Epoch 803, gen loss=3.2855968475341797,disc loss=0.21342496573925018\n",
            "Epoch 804, gen loss=3.412933588027954,disc loss=0.1502656787633896\n",
            "Epoch 805, gen loss=3.5724995136260986,disc loss=0.5372372269630432\n",
            "Epoch 806, gen loss=3.0461432933807373,disc loss=0.48022857308387756\n",
            "Epoch 807, gen loss=3.135455369949341,disc loss=0.16943709552288055\n",
            "Epoch 808, gen loss=3.0101566314697266,disc loss=0.20782124996185303\n",
            "Epoch 809, gen loss=4.3296217918396,disc loss=0.20202048122882843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-de56b5517884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-033360d4980f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mgen_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mdisc_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5bS95smv5mO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}